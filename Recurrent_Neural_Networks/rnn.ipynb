{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fae45f0",
   "metadata": {},
   "source": [
    "## IMPLEMENT A RNNS LANGUAGE MODEL FOR PREDICTING WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca72d61",
   "metadata": {},
   "source": [
    "### STEP 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb668aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.build_vocab import Vocab\n",
    "from utils.Trainer import RNNTrainer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4580c",
   "metadata": {},
   "source": [
    "### STEP 2: Download and preprocessing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef28ae4",
   "metadata": {},
   "source": [
    "2.1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from a given url\n",
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "response = requests.get(url)\n",
    "filename = \"../data/gutenberg_book.txt\"\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916125dc",
   "metadata": {},
   "source": [
    "2.2. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88070b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data, the trained tokens will be separate words\n",
    "import re\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "words = text.split()\n",
    "\n",
    "vocab = Vocab(words)\n",
    "encoded = vocab[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8993f",
   "metadata": {},
   "source": [
    "2.3. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 10 most frequent words\n",
    "freqs = vocab.token_freqs\n",
    "for item in freqs[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of words in text\n",
    "freq = [freq for word, freq in freqs]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.xlabel('token: x')\n",
    "plt.ylabel('frequency: n(x)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of words', fontweight='bold')\n",
    "plt.plot(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f877e7",
   "metadata": {},
   "source": [
    "### STEP 3: Build a specialize Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, data, num_steps):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    # Number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.num_steps\n",
    "    \n",
    "    # Method to get samples based on index\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index: index + self.num_steps]\n",
    "        y = self.data[index + 1: index + self.num_steps + 1]\n",
    "        return torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8470ca3",
   "metadata": {},
   "source": [
    "Create dataloader which supports training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f46467",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "train_data, val_data = train_test_split(encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = WordDataset(train_data, num_steps)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "val_dataset = WordDataset(val_data, num_steps)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc0c7a",
   "metadata": {},
   "source": [
    "### STEP 4: Build a language model based on RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, num_hiddens, num_layers=1):\n",
    "        super(LMRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dims)\n",
    "        self.rnn = nn.RNN(embedding_dims, num_hiddens, num_layers)\n",
    "        self.fc = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        # input -> shape: (num_steps, batch_size)\n",
    "        # x -> shape: (num_steps, batch_size, embedding_dims)\n",
    "        x = self.embed(inputs)\n",
    "        # out -> shape: (num_steps, batch_size, num_hiddens)\n",
    "        # hn -> shape: (num_layers, batch_size, num_hiddens)\n",
    "        out, hn = self.rnn(x, state)\n",
    "        # y -> shape: (num_steps, batch_size, vocab_size)\n",
    "        y = self.fc(out)\n",
    "        return y, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11fa95",
   "metadata": {},
   "source": [
    "### STEP 5: Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97afeea",
   "metadata": {},
   "source": [
    "5.1. Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6df1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dims = 128\n",
    "num_hiddens = 256\n",
    "num_layers = 5\n",
    "epochs = 10\n",
    "gradient_clip_val = 1.0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rnn_model = LMRNN(vocab_size, embedding_dims, num_hiddens, num_layers).to(device)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.25)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa497fce",
   "metadata": {},
   "source": [
    "5.2. Create a trainer for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_trainer = RNNTrainer(rnn_model, vocab_size, train_loader, val_loader, lr, num_epochs, gradient_clip_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d301ebd",
   "metadata": {},
   "source": [
    "5.3. Start traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e841df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppl, val_ppl = rnn_trainer.train_ppl, rnn_trainer.val_ppl\n",
    "rnn_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c1bac",
   "metadata": {},
   "source": [
    "5.4. Showing perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_trainer.plot()\n",
    "print(f'Training loss at the last epoch: {train_ppl[-1]}')\n",
    "print(f'Testing loss at the last epoch: {val_ppl[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce0164",
   "metadata": {},
   "source": [
    "### STEP 6: Build functions for predicting words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ae9d4",
   "metadata": {},
   "source": [
    "6.1. Build a function for predicting the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, start_text):\n",
    "    model.eval()\n",
    "    words = start_text.lower().split()\n",
    "    inputs = torch.tensor(vocab[words], dtype=torch.long).to(device)\n",
    "    # Converts inputs to (num_steps, batch_size=1)\n",
    "    inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y, _ = model(inputs)\n",
    "        last_word_digits = y[-1, 0, :]\n",
    "        predicted_idx = torch.argmax(last_word_digits).item()\n",
    "        predicted_word = vocab.to_tokens(predicted_idx)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f2bd5",
   "metadata": {},
   "source": [
    "6.2. Build a function for predicting a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03603db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_sequence(model, start_text, seq_len):\n",
    "    model.eval()\n",
    "    words = start_text.lower().split()\n",
    "    inputs = torch.tensor(vocab[words], dtype=torch.long).to(device)\n",
    "    inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(seq_len):\n",
    "            out, _ = model(inputs)\n",
    "            next_word_idx = torch.argmax(out[-1, 0, :]).item()\n",
    "            next_word = vocab.to_tokens(next_word_idx)\n",
    "            words.append(next_word)\n",
    "            # Take 'seq_len' words in 'words' to feed into the model\n",
    "            inputs = torch.tensor([vocab[w] for w in words[-num_steps:]], dtype=torch.long).reshape(-1, 1).to(device)\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c377f86",
   "metadata": {},
   "source": [
    "6.3. Implementing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I was traveling around the city\"\n",
    "print(predict_next_word(rnn_model, prompt))\n",
    "print(predict_next_sequence(rnn_model, prompt, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef1d6d",
   "metadata": {},
   "source": [
    "### STEP 7: Testing LSTM and GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91806af3",
   "metadata": {},
   "source": [
    "7.1. Implement LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0154dc1",
   "metadata": {},
   "source": [
    "7.1.1. Build a LSTM language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, num_hiddens, num_layers=1):\n",
    "        super(LMLSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dims)\n",
    "        self.lstm = nn.LSTM(embedding_dims, num_hiddens, num_layers)\n",
    "        self.fc = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        # input -> shape: (num_steps, batch_size)\n",
    "        # x -> shape: (num_steps, batch_size, embedding_dims)\n",
    "        x = self.embed(inputs)\n",
    "        # out -> shape: (num_steps, batch_size, num_hiddens)\n",
    "        # hn -> shape: (num_layers, batch_size, num_hiddens)\n",
    "        out, hn = self.lstm(x, state)\n",
    "        # y -> shape: (num_steps, batch_size, vocab_size)\n",
    "        y = self.fc(out)\n",
    "        return y, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8515595",
   "metadata": {},
   "source": [
    "7.1.2. Build a trainer class for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LMLSTM(vocab_size, embedding_dims, num_hiddens, num_layers).to(device)\n",
    "lstm_trainer = RNNTrainer(lstm_model, vocab_size, train_loader, val_loader, lr, num_epochs, gradient_clip_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7e73f",
   "metadata": {},
   "source": [
    "7.1.3. Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57925756",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306853c",
   "metadata": {},
   "source": [
    "7.1.4. Plot perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7fdd7",
   "metadata": {},
   "source": [
    "7.1.5. Predicting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f10ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I was traveling around the city\"\n",
    "print(predict_next_word(lstm_model, prompt))\n",
    "print(predict_next_sequence(lstm_model, prompt, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5475e31",
   "metadata": {},
   "source": [
    "7.2. Implement GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83966de4",
   "metadata": {},
   "source": [
    "7.2.1. Build a GRU language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291abded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, num_hiddens, num_layers=1):\n",
    "        super(LMGRU, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dims)\n",
    "        self.gru = nn.GRU(embedding_dims, num_hiddens, num_layers)\n",
    "        self.fc = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        # input -> shape: (num_steps, batch_size)\n",
    "        # x -> shape: (num_steps, batch_size, embedding_dims)\n",
    "        x = self.embed(inputs)\n",
    "        # out -> shape: (num_steps, batch_size, num_hiddens)\n",
    "        # hn -> shape: (num_layers, batch_size, num_hiddens)\n",
    "        out, hn = self.gru(x, state)\n",
    "        # y -> shape: (num_steps, batch_size, vocab_size)\n",
    "        y = self.fc(out)\n",
    "        return y, hn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88296a5",
   "metadata": {},
   "source": [
    "7.2.2. Build a trainer class for GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = LMGRU(vocab_size, embedding_dims, num_hiddens, num_layers).to(device)\n",
    "gru_trainer = RNNTrainer(lstm_model, vocab_size, train_loader, val_loader, lr, num_epochs, gradient_clip_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635a622",
   "metadata": {},
   "source": [
    "7.2.3. Train GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ec282",
   "metadata": {},
   "source": [
    "7.2.4. Plot perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d74dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf95a90",
   "metadata": {},
   "source": [
    "7.2.5. Predicting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I was traveling around the city\"\n",
    "print(predict_next_word(gru_model, prompt))\n",
    "print(predict_next_sequence(gru_model, prompt, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
